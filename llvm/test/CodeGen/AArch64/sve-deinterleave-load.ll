; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=aarch64--linux-gnu -mattr=+sve < %s | FileCheck %s

%struct.xyzt = type { i32, i32, i32, i32 }

define dso_local void @loop_xyzt(ptr noalias nocapture noundef writeonly %dst, ptr nocapture noundef readonly %a, ptr nocapture noundef readonly %b) {
; CHECK-LABEL: loop_xyzt:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ptrue p0.s
; CHECK-NEXT:    cntw x10
; CHECK-NEXT:    mov x8, xzr
; CHECK-NEXT:    mov w9, #1024 // =0x400
; CHECK-NEXT:    neg x10, x10
; CHECK-NEXT:    rdvl x11, #4
; CHECK-NEXT:  .LBB0_1: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    add x12, x1, x8
; CHECK-NEXT:    adds x9, x9, x10
; CHECK-NEXT:    ld4w { z0.s - z3.s }, p0/z, [x12]
; CHECK-NEXT:    add x12, x2, x8
; CHECK-NEXT:    ld4w { z4.s - z7.s }, p0/z, [x12]
; CHECK-NEXT:    add x12, x0, x8
; CHECK-NEXT:    add x8, x8, x11
; CHECK-NEXT:    add z16.s, z4.s, z0.s
; CHECK-NEXT:    sub z17.s, z1.s, z5.s
; CHECK-NEXT:    movprfx z18, z2
; CHECK-NEXT:    lsl z18.s, p0/m, z18.s, z6.s
; CHECK-NEXT:    movprfx z19, z3
; CHECK-NEXT:    asr z19.s, p0/m, z19.s, z7.s
; CHECK-NEXT:    st4w { z16.s - z19.s }, p0, [x12]
; CHECK-NEXT:    b.ne .LBB0_1
; CHECK-NEXT:  // %bb.2: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 2
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %entry
  %index = phi i64 [ 0, %entry ], [ %index.next, %vector.body ]
  %2 = getelementptr inbounds %struct.xyzt, ptr %a, i64 %index
  %wide.vec = load <vscale x 16 x i32>, ptr %2, align 4
  %root.strided.vec = tail call { <vscale x 8 x i32>, <vscale x 8 x i32> } @llvm.experimental.vector.deinterleave2.nxv16i32(<vscale x 16 x i32> %wide.vec)
  %3 = extractvalue { <vscale x 8 x i32>, <vscale x 8 x i32> } %root.strided.vec, 0
  %4 = extractvalue { <vscale x 8 x i32>, <vscale x 8 x i32> } %root.strided.vec, 1
  %root.strided.vec55 = tail call { <vscale x 4 x i32>, <vscale x 4 x i32> } @llvm.experimental.vector.deinterleave2.nxv8i32(<vscale x 8 x i32> %3)
  %5 = extractvalue { <vscale x 4 x i32>, <vscale x 4 x i32> } %root.strided.vec55, 0
  %6 = extractvalue { <vscale x 4 x i32>, <vscale x 4 x i32> } %root.strided.vec55, 1
  %root.strided.vec56 = tail call { <vscale x 4 x i32>, <vscale x 4 x i32> } @llvm.experimental.vector.deinterleave2.nxv8i32(<vscale x 8 x i32> %4)
  %7 = extractvalue { <vscale x 4 x i32>, <vscale x 4 x i32> } %root.strided.vec56, 0
  %8 = extractvalue { <vscale x 4 x i32>, <vscale x 4 x i32> } %root.strided.vec56, 1
  %9 = getelementptr inbounds %struct.xyzt, ptr %b, i64 %index
  %wide.vec57 = load <vscale x 16 x i32>, ptr %9, align 4
  %root.strided.vec58 = tail call { <vscale x 8 x i32>, <vscale x 8 x i32> } @llvm.experimental.vector.deinterleave2.nxv16i32(<vscale x 16 x i32> %wide.vec57)
  %10 = extractvalue { <vscale x 8 x i32>, <vscale x 8 x i32> } %root.strided.vec58, 0
  %11 = extractvalue { <vscale x 8 x i32>, <vscale x 8 x i32> } %root.strided.vec58, 1
  %root.strided.vec59 = tail call { <vscale x 4 x i32>, <vscale x 4 x i32> } @llvm.experimental.vector.deinterleave2.nxv8i32(<vscale x 8 x i32> %10)
  %12 = extractvalue { <vscale x 4 x i32>, <vscale x 4 x i32> } %root.strided.vec59, 0
  %13 = extractvalue { <vscale x 4 x i32>, <vscale x 4 x i32> } %root.strided.vec59, 1
  %root.strided.vec60 = tail call { <vscale x 4 x i32>, <vscale x 4 x i32> } @llvm.experimental.vector.deinterleave2.nxv8i32(<vscale x 8 x i32> %11)
  %14 = extractvalue { <vscale x 4 x i32>, <vscale x 4 x i32> } %root.strided.vec60, 0
  %15 = extractvalue { <vscale x 4 x i32>, <vscale x 4 x i32> } %root.strided.vec60, 1
  %16 = add nsw <vscale x 4 x i32> %12, %5
  %17 = sub nsw <vscale x 4 x i32> %6, %13
  %18 = shl <vscale x 4 x i32> %7, %14
  %19 = ashr <vscale x 4 x i32> %8, %15
  %20 = getelementptr inbounds %struct.xyzt, ptr %dst, i64 %index
  %interleaved.vec = tail call <vscale x 8 x i32> @llvm.experimental.vector.interleave2.nxv8i32(<vscale x 4 x i32> %16, <vscale x 4 x i32> %17)
  %interleaved.vec61 = tail call <vscale x 8 x i32> @llvm.experimental.vector.interleave2.nxv8i32(<vscale x 4 x i32> %18, <vscale x 4 x i32> %19)
  %interleaved.vec62 = tail call <vscale x 16 x i32> @llvm.experimental.vector.interleave2.nxv16i32(<vscale x 8 x i32> %interleaved.vec, <vscale x 8 x i32> %interleaved.vec61)
  store <vscale x 16 x i32> %interleaved.vec62, ptr %20, align 4
  %index.next = add nuw i64 %index, %1
  %21 = icmp eq i64 %index.next, 1024
  br i1 %21, label %for.cond.cleanup, label %vector.body

for.cond.cleanup:                                 ; preds = %vector.body
  ret void
}

define {<vscale x 4 x i32>, <vscale x 4 x i32>} @vector_deinterleave_load_nxv4i32_nxv8i32(ptr %p) {
; CHECK-LABEL: vector_deinterleave_load_nxv4i32_nxv8i32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p0.s
; CHECK-NEXT:    ld2w { z0.s, z1.s }, p0/z, [x0]
; CHECK-NEXT:    ret
  %vec = load <vscale x 8 x i32>, ptr %p
  %retval = call {<vscale x 4 x i32>, <vscale x 4 x i32>} @llvm.experimental.vector.deinterleave2.nxv8i32(<vscale x 8 x i32> %vec)
  ret {<vscale x 4 x i32>, <vscale x 4 x i32>} %retval
}
